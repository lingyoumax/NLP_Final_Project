{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3346fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T12:00:16.340670Z",
     "start_time": "2024-03-03T12:00:16.333356Z"
    }
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "model = FlagModel('bge_large_fin',\n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=True)\n",
    "\n",
    "pc = Pinecone(api_key=\"621f7574-8c97-4f46-8c5e-186dd099d33b\")\n",
    "Index = pc.Index(\"bge-fin\")\n",
    "\n",
    "\n",
    "def search_arxiv_texts(query):\n",
    "    query_vector = model.encode_queries([query])[0].tolist()\n",
    "\n",
    "    response = Index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=1,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    arxiv_texts = [str(match['metadata']['arxiv_text']) for match in response['matches']]\n",
    "    pmid = [int(match['metadata']['pmid']) for match in response['matches']]\n",
    "    # ans = {}\n",
    "    # for i in range(5):\n",
    "    #     ans[pmid[i]] = arxiv_texts[i]\n",
    "\n",
    "    return [pmid[0], arxiv_texts[0]]\n",
    "\n",
    "\n",
    "# query = \"What was the purpose of the US Food and Drug Administration-cosponsored forum on laser-based imaging?\"\n",
    "# top_arxiv_texts = search_arxiv_texts(query)\n",
    "# print(top_arxiv_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6b95c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from typing import List\n",
    "import yake\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "kw_extractor = yake.KeywordExtractor(n=1,\n",
    "                                     dedupLim=0.9,\n",
    "                                     top=10,\n",
    "                                     features=None)\n",
    "\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, corpus: List[List[str]], k1=1.5, b=0.95):\n",
    "        self.corpus = corpus\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.documents_number = len(corpus)\n",
    "        self.avgdl = sum(len(document) for document in corpus) / self.documents_number\n",
    "        self.df = self._calculate_df()\n",
    "        self.idf = self._calculate_idf()\n",
    "\n",
    "    def _calculate_df(self):\n",
    "        df = {}\n",
    "        for document in self.corpus:\n",
    "            for word in set(document):\n",
    "                df[word] = df.get(word, 0) + 1\n",
    "        return df\n",
    "\n",
    "    def _calculate_idf(self):\n",
    "        idf = {}\n",
    "        for word, freq in self.df.items():\n",
    "            idf[word] = math.log((self.documents_number - freq + 0.5) / (freq + 0.5) + 1)\n",
    "        return idf\n",
    "\n",
    "    def _score(self, document, query):\n",
    "        score = 0.0\n",
    "        for word in query:\n",
    "            if word in self.df:\n",
    "                idf = self.idf[word]\n",
    "                term_freq = document.count(word)\n",
    "                score += (idf * term_freq * (self.k1 + 1)) / (\n",
    "                        term_freq + self.k1 * (1 - self.b + self.b * len(document) / self.avgdl))\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        scores = []\n",
    "        for index, document in enumerate(self.corpus):\n",
    "            score = self._score(document, query)\n",
    "            scores.append((index, score))\n",
    "        return scores\n",
    "\n",
    "\n",
    "def search(query: str, df, keywords, bm25, top_k):\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query.extend(keywords)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    result = {}\n",
    "    if top_k == 1:\n",
    "        for doc_index, score in sorted_scores:\n",
    "            return [df.iloc[doc_index]['PMID'], df.iloc[doc_index]['chunk_text']]\n",
    "        \n",
    "    for doc_index, score in sorted_scores:\n",
    "        pmid = df.iloc[doc_index]['PMID']\n",
    "        result[pmid] = score\n",
    "    return list(result.keys())\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    keywords_set = {word for word, _ in keywords}\n",
    "    return list(set(keywords_set))\n",
    "\n",
    "\n",
    "def query_pre_process(query):\n",
    "    proper_nouns = extract_keywords(query)\n",
    "    proper_nouns = [word for word in proper_nouns if word.lower() not in ENGLISH_STOP_WORDS] * 1\n",
    "    return proper_nouns\n",
    "\n",
    "df = pd.read_csv('./PubmedDataSet.csv')\n",
    "texts = df['Abstract'].tolist()\n",
    "tokenized_texts = [doc.split() for doc in texts]\n",
    "bm25_abstract = BM25(tokenized_texts)\n",
    "df2 = pd.read_csv('./splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "def weightBM25(query):\n",
    "    keywords = query_pre_process(query)\n",
    "    result_pmids_scores = search(query, df, keywords, bm25_abstract, top_k=30)\n",
    "    mask = df2['PMID'].isin(result_pmids_scores)\n",
    "    df_t = df2[mask]\n",
    "    texts = df_t['chunk_text'].tolist()\n",
    "    tokenized_texts = [doc.split() for doc in texts]\n",
    "    \n",
    "    bm25_chunk = BM25(tokenized_texts)\n",
    "    result_chunk = search(query, df_t, keywords, bm25_chunk, top_k=1)\n",
    "    return result_chunk\n",
    "    \n",
    "    # res = []\n",
    "    # for id in result_pmids_scores:\n",
    "    #     res = [id, df_t[df_t[\"PMID\"] == id]['chunk_text'].iloc[0]]\n",
    "    # return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7211cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(question):    \n",
    "    res_semantic = search_arxiv_texts(question)[1]\n",
    "    res_lex = weightBM25(question)[1]\n",
    "    question += ' ' + res_semantic + ' ' + res_lex\n",
    "\n",
    "    res_semantic = search_arxiv_texts(question)[0]\n",
    "    res_lex = weightBM25(question)[0]\n",
    "    \n",
    "    intersectPMID = [res_semantic, res_lex]\n",
    "    # print(intersectPMID)\n",
    "    return intersectPMID\n",
    "    # if len(intersectPMID) != 0:\n",
    "    #     for id in res_semantic:\n",
    "    #         if id in intersectPMID:\n",
    "    #             return id\n",
    "    #             break\n",
    "    # else:\n",
    "    #     return res_semantic[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "233e3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df_test = pd.read_csv('./evaluation.csv')\n",
    "test = []\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    small_list = [row[\"Question\"], row[\"PMID\"]]\n",
    "    test.append(small_list)\n",
    "random.seed(42)\n",
    "random.shuffle(test)\n",
    "test = test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fe2f2de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [32:37<00:00, 19.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall of mixed search algorithm 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for question, pmid in tqdm(test):\n",
    "    top_pmids = retrieval(question)\n",
    "    if pmid in top_pmids:\n",
    "        t += 1\n",
    "print(f'top1-accuracy of PRF search {t / len(test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
