{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pandas as pd\n",
    "# 假设data和labels是你的数据和标签\n",
    "\n",
    "df = pd.read_csv('squad_questions_and_types_balanced_500.csv')\n",
    "data=df.loc[:,\"Question\"]\n",
    "label=df.loc[:, \"Confirmation\":\"Complex\"]\n",
    "# 分割数据集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data, label, test_size=.2)\n",
    "\n",
    "train_texts=train_texts.values.tolist()\n",
    "train_labels=train_labels.values.tolist()\n",
    "val_texts=val_texts.values.tolist()\n",
    "val_labels=val_labels.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 初始化Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 编码数据\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# 转换为torch tensors\n",
    "train_seq = torch.tensor(train_encodings['input_ids'])\n",
    "train_mask = torch.tensor(train_encodings['attention_mask'])\n",
    "train_y = torch.tensor(train_labels, dtype=torch.float)\n",
    "\n",
    "val_seq = torch.tensor(val_encodings['input_ids'])\n",
    "val_mask = torch.tensor(val_encodings['attention_mask'])\n",
    "val_y = torch.tensor(val_labels, dtype=torch.float)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500\n",
    "early_stopping_patience = 10  # 设置容忍度（即连续多少个epoch没有改善就停止）\n",
    "best_loss = np.Inf\n",
    "early_stopping_counter = 0  # 用于跟踪没有改善的epoch数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "e:\\InstalledFile\\Anaconda\\envs\\NLP\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 15 train loss: 0.053178,val loss0.191471:   3%|▎         | 16/500 [04:24<2:13:09, 16.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 16 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from model import BERTMultiLabelBinaryClassification\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTMultiLabelBinaryClassification(num_labels=6)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=5,  # 不使用warmup步骤\n",
    "                                            num_training_steps=len(train_dataloader)*epochs)\n",
    "\n",
    "\n",
    "progress_bar = tqdm(range(epochs), desc='Training Progress')\n",
    "# 训练模型的简化代码示例\n",
    "model=model.to(device)\n",
    "for epoch in progress_bar:  # 循环训练3个epoch\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2]}\n",
    "        loss = model(**inputs)\n",
    "\n",
    "        #loss = loss_fn(outputs, inputs['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    total_loss=total_loss / len(train_dataloader)\n",
    "\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels': batch[2]}\n",
    "            loss = model(**inputs)\n",
    "            #batch_loss = loss_fn(outputs, inputs['labels'])\n",
    "            val_loss += loss.item()\n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        # 保存最好的模型状态\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        early_stopping_counter = 0  # 重置早停计数器\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "    \n",
    "    progress_bar.set_description(\"Epoch {:} train loss: {:.6f},val loss{:.6f}\".format(epoch,total_loss,val_loss))\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.0000 Precision:0.2000 Recall:0.7282 F1 Score:0.3138 ROC AUC:0.3111\n",
      "Confirmation Accuracy:0.1511 Precision:0.1511 Recall:0.1511 F1 Score:0.1511 ROC AUC:0.0627\n",
      "Factoid Accuracy:0.7104 Precision:0.7104 Recall:0.7104 F1 Score:0.7104 ROC AUC:0.8150\n",
      "List Accuracy:0.0325 Precision:0.0325 Recall:0.0325 F1 Score:0.0325 ROC AUC:0.2237\n",
      "Causal Accuracy:0.9151 Precision:0.9151 Recall:0.9151 F1 Score:0.9151 ROC AUC:0.9663\n",
      "Hypothetical Accuracy:0.0275 Precision:0.0275 Recall:0.0275 F1 Score:0.0275 ROC AUC:0.1828\n",
      "Complex Accuracy:0.1124 Precision:0.1124 Recall:0.1124 F1 Score:0.1124 ROC AUC:0.5195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 存储预测和真实标签\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# 禁用梯度计算\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1]\n",
    "        }\n",
    "        labels = batch[2]\n",
    "\n",
    "        # 获取模型输出\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # 将输出移动到CPU并转换为numpy数组\n",
    "        logits = outputs.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "        # 存储预测和真实标签\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "# 计算性能指标\n",
    "predictions = np.vstack(predictions)  # 将预测结果堆叠起来\n",
    "true_labels = np.vstack(true_labels)  # 将真实标签堆叠起来\n",
    "\n",
    "# 计算每个标签的预测值\n",
    "pred_labels=np.zeros_like(predictions)\n",
    "split=[0.5,0.5,0.5,0.5,0.5,0.5]\n",
    "for i in range(len(split)):\n",
    "    predictions[:,i]=predictions[:,i]/predictions[:,i].max()\n",
    "    pred_labels[:,i]=(predictions[:,i] > split[i]).astype(int)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='micro')\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "roc_auc = roc_auc_score(true_labels, predictions, average='micro')\n",
    "\n",
    "print(\"Accuracy:{:.4f} Precision:{:.4f} Recall:{:.4f} F1 Score:{:.4f} ROC AUC:{:.4f}\".format(accuracy,precision,recall,f1,roc_auc))\n",
    "\n",
    "list_name=['Confirmation','Factoid','List','Causal','Hypothetical','Complex']\n",
    "for i in range(6):\n",
    "# 计算指标\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels[:,i], pred_labels[:,i], average='micro')\n",
    "    accuracy = accuracy_score(true_labels[:,i], pred_labels[:,i])\n",
    "    roc_auc = roc_auc_score(true_labels[:,i], predictions[:,i], average='micro')\n",
    "\n",
    "    print(\"{:} Accuracy:{:.4f} Precision:{:.4f} Recall:{:.4f} F1 Score:{:.4f} ROC AUC:{:.4f}\".format(list_name[i],accuracy,precision,recall,f1,roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_state_dict_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Who commanded the Prussian 12 Cavalry Brigade?\"\n",
    "\n",
    "sentence_encodings = tokenizer(sentence, truncation=True, padding=True, max_length=128)\n",
    "sentence_seq = torch.tensor(sentence_encodings['input_ids'])\n",
    "sentence_mask = torch.tensor(sentence_encodings['attention_mask'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model.cpu()\n",
    "    inputs = {\n",
    "            'input_ids':sentence_seq.unsqueeze(0),\n",
    "            'attention_mask':sentence_mask.unsqueeze(0)\n",
    "        }\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.detach().cpu().numpy()\n",
    "pred_labels=(logits > 0.5).astype(int)\n",
    "print(pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
