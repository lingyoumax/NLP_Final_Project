{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 58742/58742 [00:52<00:00, 1121.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "df = pd.read_csv('PubmedData.csv').drop('Keywords', axis=1)\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=250)\n",
    "#text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "#text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "#text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "\n",
    "splitted = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    chunks = text_splitter.split_text(row['Abstract'])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        new_row = row.drop('Abstract') \n",
    "        new_row['chunk_id'] = i\n",
    "        new_row['chunk_text'] = chunk\n",
    "        splitted.append(new_row)\n",
    "\n",
    "splitted_df = pd.DataFrame(splitted)\n",
    "splitted_df.to_csv('splitted_pubmed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = FlagModel('BAAI/bge-large-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=False) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "index_name = 'abstracts_bge'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "# mapping = {\n",
    "#     \"mappings\": {\n",
    "#         \"properties\": {\n",
    "#             \"pmid\": {\n",
    "#                 \"type\": \"keyword\",  \n",
    "#             },\n",
    "#             \"title\": {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"analyzer\": \"standard\",  \n",
    "#             },\n",
    "#             \"vector\": {\n",
    "#                 \"type\": \"knn_vector\",  \n",
    "#                 \"dimension\": 1024\n",
    "#             },\n",
    "#             \"publishedDate\": {\n",
    "#                 \"type\": \"date\",  # date type for publication date\n",
    "#             },\n",
    "#             \"authors\": {\n",
    "#                 \"type\": \"text\",  # text field for author names\n",
    "#             },\n",
    "#             \"text_chunk_id\": {\n",
    "#                 \"type\": \"integer\",\n",
    "#             },\n",
    "#             \"arxiv_text\": {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"analyzer\": \"standard\",\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "# }\n",
    "# client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "# for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "#     pmid = row['PMID']\n",
    "#     publishedDate = row['PubDate']\n",
    "#     if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "#         publishedDate = None\n",
    "#     title = row['ArticleTitle']\n",
    "#     authors_list = row[\"Authors\"].split(',')\n",
    "#     authors = [author.strip().lower() for author in authors_list]\n",
    "#     chunk_id = row['chunk_id']\n",
    "#     chunk_text = row['chunk_text']\n",
    "#     embedding = model.encode(chunk_text).tolist()\n",
    "#     client.index(index_name, {\n",
    "#         \"pmid\": pmid,\n",
    "#         \"title\": title,\n",
    "#         \"vector\": embedding,\n",
    "#         \"publishedDate\": publishedDate,\n",
    "#         \"authors\": authors,\n",
    "#         \"text_chunk_id\": chunk_id,\n",
    "#         \"arxiv_text\": chunk_text,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:31<00:00,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PMID found in top 5 results for 71.89999999999999% of queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "df_qa = pd.read_csv('qap.csv').drop('Answer', axis=1)\n",
    "data_list = df_qa.values.tolist()\n",
    "random.shuffle(data_list)\n",
    "data_list = data_list[:1000]\n",
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    embedding = model.encode_queries(query).tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index=index_name, body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62372/62372 [14:44<00:00, 70.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import LLMEmbedder\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = LLMEmbedder('BAAI/llm-embedder', use_fp16=False)\n",
    "task = \"qa\"\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "index_name = 'abstracts_llm-embedder'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 768\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    embedding = model.encode_keys(chunk_text, task=task).tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:06<00:00,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PMID found in top 5 results for 64.8% of queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_qa = pd.read_csv('qap.csv').drop('Answer', axis=1)\n",
    "data_list = df_qa.values.tolist()\n",
    "data_list = data_list[:1000]\n",
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    embedding = model.encode_queries(query, task=task).tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index=index_name, body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "from angle_emb import AnglE, Prompts\n",
    "\n",
    "\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "angle.set_prompt(prompt=Prompts.C)\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "index_name = 'abstracts_uae'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 1024\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    vecs = angle.encode({'text': chunk_text}, to_numpy=True)\n",
    "    embedding = vecs[0].tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df_qa = pd.read_csv('qap.csv').drop('Answer', axis=1)\n",
    "data_list = df_qa.values.tolist()\n",
    "random.shuffle(data_list)\n",
    "data_list = data_list[:1000]\n",
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    vecs = angle.encode({'text': query}, to_numpy=True)\n",
    "    embedding = vecs[0].tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index=index_name, body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "qap = pd.read_csv('qap.csv')\n",
    "splitted = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "pmid_counts = splitted['PMID'].value_counts()\n",
    "splitted_filtered = splitted[splitted['PMID'].map(pmid_counts) == 1]\n",
    "\n",
    "merged_df = qap.merge(splitted_filtered, how='left', left_on='PMID', right_on='PMID')\n",
    "result_df = merged_df[['Question', 'Answer', 'chunk_text']]\n",
    "result_df.to_csv('qaa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  96%|██████████████████████████████████████████████████████████████████████████████████████████▏   | 8576/8940 [00:38<00:04, 88.62it/s]Created a chunk of size 505, which is longer than the specified 500\n",
      "Processing rows:  97%|██████████████████████████████████████████████████████████████████████████████████████████▉   | 8644/8940 [01:38<00:32,  9.06it/s]Created a chunk of size 665, which is longer than the specified 500\n",
      "Processing rows:  97%|███████████████████████████████████████████████████████████████████████████████████████████▌  | 8712/8940 [02:38<01:52,  2.03it/s]Created a chunk of size 866, which is longer than the specified 500\n",
      "Processing rows:  98%|███████████████████████████████████████████████████████████████████████████████████████████▊  | 8733/8940 [02:58<02:13,  1.55it/s]Created a chunk of size 550, which is longer than the specified 500\n",
      "Processing rows:  98%|████████████████████████████████████████████████████████████████████████████████████████████▎ | 8776/8940 [03:38<02:16,  1.20it/s]Created a chunk of size 565, which is longer than the specified 500\n",
      "Processing rows:  98%|████████████████████████████████████████████████████████████████████████████████████████████▌ | 8799/8940 [03:59<02:01,  1.16it/s]Created a chunk of size 724, which is longer than the specified 500\n",
      "Processing rows:  99%|█████████████████████████████████████████████████████████████████████████████████████████████▏| 8857/8940 [04:52<01:14,  1.11it/s]Created a chunk of size 602, which is longer than the specified 500\n",
      "Created a chunk of size 1037, which is longer than the specified 500\n",
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████████████████████████████████▋| 8905/8940 [05:33<00:30,  1.14it/s]Created a chunk of size 726, which is longer than the specified 500\n",
      "Processing rows: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 8940/8940 [06:03<00:00, 24.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import voyageai\n",
    "import os\n",
    "\n",
    "vo = voyageai.Client(api_key=\"pa-VidMEe9WZqvQl1nxwnR7PmW4uDGmHia0lzVyX0ftboo\")\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "\n",
    "def clean_text(text):\n",
    "    # 移除换行符和多余的空格\n",
    "    return ' '.join(text.replace('\\n', ' ').split())\n",
    "\n",
    "input_file = 'qad.csv'\n",
    "output_file = 'updated_qad.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    processed_df = pd.read_csv(output_file)\n",
    "else:\n",
    "    processed_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "processed_indices = set(processed_df.index)\n",
    "\n",
    "# 遍历每一行\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "    if index in processed_indices:\n",
    "        continue\n",
    "    # 分割chunk_text\n",
    "    chunks = [clean_text(chunk) for chunk in text_splitter.split_text(row['chunk_text'])]\n",
    "\n",
    "    # 获取每个chunk的嵌入向量\n",
    "    chunk_vectors = vo.embed(chunks, model=\"voyage-lite-01-instruct\", input_type=\"document\").embeddings\n",
    "\n",
    "    # 获取Question的嵌入向量\n",
    "    question_vector = vo.embed(row['Question'], model=\"voyage-lite-01-instruct\", input_type=\"document\").embeddings[0]\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    similarities = [cosine_similarity([question_vector], [chunk_vector])[0][0] for chunk_vector in chunk_vectors]\n",
    "\n",
    "    # 获取Top 3相似的chunks\n",
    "    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:3]\n",
    "    top_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "    # 更新DataFrame\n",
    "    df.at[index, 'chunk_text'] = top_chunks[0] if top_chunks else None\n",
    "    df.at[index, 'Top2'] = top_chunks[1] if len(top_chunks) > 1 else None\n",
    "    df.at[index, 'Top3'] = top_chunks[2] if len(top_chunks) > 2 else None\n",
    "\n",
    "    df.iloc[[index]].to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('updated_qad.csv')\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    query = row['Question']\n",
    "    pos = [row['chunk_text']]\n",
    "    neg = random.choice(list(df[df.index != index]['chunk_text']))\n",
    "    train_data.append({\"query\": query, \"pos\": pos, \"neg\": [neg]})\n",
    "\n",
    "with open('train_data.jsonl', 'w') as outfile:\n",
    "    for entry in train_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "with open('candidate_pool.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for text in df['chunk_text']:\n",
    "        json_obj = json.dumps({\"text\": text})\n",
    "        file.write(json_obj + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 -m FlagEmbedding.baai_general_embedding.finetune.hn_mine \\\n",
    "--model_name_or_path BAAI/bge-large-en-v1.5  \\\n",
    "--input_file train_data.jsonl \\\n",
    "--output_file qa_finetune_data_minedHN.jsonl \\\n",
    "--range_for_sampling 50-300 \\\n",
    "--candidate_pool candidate_pool.jsonl \\\n",
    "--use_gpu_for_searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "neg_count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        neg_count += len(data.get('neg', []))\n",
    "        break\n",
    "\n",
    "neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'query': 119.96017897091723\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "total_length = 0\n",
    "count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        query = data.get('query', '')\n",
    "        total_length += len(query)\n",
    "        count += 1\n",
    "average_length = total_length / count if count > 0 else 0\n",
    "\n",
    "print(f\"Average length of 'query': {average_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'passage': 408.1756431767338\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "total_length = 0\n",
    "count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        for passage in data.get('pos', []):\n",
    "            total_length += len(passage)\n",
    "            count += 1\n",
    "\n",
    "        for passage in data.get('neg', []):\n",
    "            total_length += len(passage)\n",
    "            count += 1\n",
    "\n",
    "average_length = total_length / count if count > 0 else 0\n",
    "\n",
    "print(f\"Average length of 'passage': {average_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchrun --nproc_per_node 1 \\\n",
    "-m FlagEmbedding.baai_general_embedding.finetune.run \\\n",
    "--output_dir bge_large_fin \\\n",
    "--model_name_or_path BAAI/bge-large-en-v1.5 \\\n",
    "--train_data qa_finetune_data_minedHN.jsonl \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--dataloader_drop_last True \\\n",
    "--normlized True \\\n",
    "--temperature 0.02 \\\n",
    "--query_max_len 120 \\\n",
    "--passage_max_len 408 \\\n",
    "--train_group_size 5 \\\n",
    "--negatives_cross_device \\\n",
    "--logging_steps 100 \\\n",
    "--query_instruction_for_retrieval \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 285186/285186 [1:26:13<00:00, 55.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = FlagModel('bge_large_fin', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=False) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "index_name = 'abstracts_bge_fin1'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 1024\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    embedding = model.encode(chunk_text).tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [08:53<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PMID found in top 5 results for 75.5% of queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "df_qa = pd.read_csv('qap.csv').drop('Answer', axis=1)\n",
    "data_list = df_qa.values.tolist()\n",
    "random.shuffle(data_list)\n",
    "data_list = data_list[:1000]\n",
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    embedding = model.encode_queries(query).tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index=index_name, body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from angle_emb import AnglE, AngleDataTokenizer, Prompts\n",
    "import json\n",
    "\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "angle.set_prompt(prompt=Prompts.C)\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        data = [{'text': json.loads(line)['query'], \n",
    "                        'positive': pos, \n",
    "                        'negative': neg} \n",
    "                for line in lines \n",
    "                for pos in json.loads(line)['pos'] \n",
    "                for neg in json.loads(line)['neg']]\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "train_ds = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "valid_ds = Dataset.from_pandas(pd.DataFrame(valid_data))\n",
    "test_ds = Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "\n",
    "train_ds = train_ds.shuffle().map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "valid_ds = valid_ds.map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "test_ds = test_ds.map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "\n",
    "# 4. fit\n",
    "angle.fit(\n",
    "    train_ds=train_ds,\n",
    "    valid_ds=valid_ds,\n",
    "    output_dir='ckpts/sts-b',\n",
    "    batch_size=8,\n",
    "    epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=100,\n",
    "    eval_steps=1000,\n",
    "    warmup_steps=0,\n",
    "    gradient_accumulation_steps=1,\n",
    "    loss_kwargs={\n",
    "        'w1': 1.0,\n",
    "        'w2': 1.0,\n",
    "        'w3': 1.0,\n",
    "        'cosine_tau': 20,\n",
    "        'ibn_tau': 20,\n",
    "        'angle_tau': 1.0\n",
    "    },\n",
    "    fp16=True,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# 5. evaluate\n",
    "corrcoef, accuracy = angle.evaluate(test_ds, device=angle.device)\n",
    "print('corrcoef:', corrcoef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
