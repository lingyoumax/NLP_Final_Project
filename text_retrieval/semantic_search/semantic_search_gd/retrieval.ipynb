{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = FlagModel('BAAI/bge-large-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=False) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "index_name = 'abstracts_bge'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "# mapping = {\n",
    "#     \"mappings\": {\n",
    "#         \"properties\": {\n",
    "#             \"pmid\": {\n",
    "#                 \"type\": \"keyword\",  \n",
    "#             },\n",
    "#             \"title\": {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"analyzer\": \"standard\",  \n",
    "#             },\n",
    "#             \"vector\": {\n",
    "#                 \"type\": \"knn_vector\",  \n",
    "#                 \"dimension\": 1024\n",
    "#             },\n",
    "#             \"publishedDate\": {\n",
    "#                 \"type\": \"date\",  # date type for publication date\n",
    "#             },\n",
    "#             \"authors\": {\n",
    "#                 \"type\": \"text\",  # text field for author names\n",
    "#             },\n",
    "#             \"text_chunk_id\": {\n",
    "#                 \"type\": \"integer\",\n",
    "#             },\n",
    "#             \"arxiv_text\": {\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"analyzer\": \"standard\",\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "# }\n",
    "# client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "# for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "#     pmid = row['PMID']\n",
    "#     publishedDate = row['PubDate']\n",
    "#     if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "#         publishedDate = None\n",
    "#     title = row['ArticleTitle']\n",
    "#     authors_list = row[\"Authors\"].split(',')\n",
    "#     authors = [author.strip().lower() for author in authors_list]\n",
    "#     chunk_id = row['chunk_id']\n",
    "#     chunk_text = row['chunk_text']\n",
    "#     embedding = model.encode(chunk_text).tolist()\n",
    "#     client.index(index_name, {\n",
    "#         \"pmid\": pmid,\n",
    "#         \"title\": title,\n",
    "#         \"vector\": embedding,\n",
    "#         \"publishedDate\": publishedDate,\n",
    "#         \"authors\": authors,\n",
    "#         \"text_chunk_id\": chunk_id,\n",
    "#         \"arxiv_text\": chunk_text,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62372/62372 [14:44<00:00, 70.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import LLMEmbedder\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = LLMEmbedder('BAAI/llm-embedder', use_fp16=False)\n",
    "task = \"qa\"\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "index_name = 'abstracts_llm-embedder'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 768\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    embedding = model.encode_keys(chunk_text, task=task).tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "from angle_emb import AnglE, Prompts\n",
    "\n",
    "\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "angle.set_prompt(prompt=Prompts.C)\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "index_name = 'abstracts_uae'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 1024\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    vecs = angle.encode({'text': chunk_text}, to_numpy=True)\n",
    "    embedding = vecs[0].tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df_qa = pd.read_csv('qap.csv').drop('Answer', axis=1)\n",
    "data_list = df_qa.values.tolist()\n",
    "random.shuffle(data_list)\n",
    "data_list = data_list[:1000]\n",
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    vecs = angle.encode({'text': query}, to_numpy=True)\n",
    "    embedding = vecs[0].tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index=index_name, body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "neg_count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        neg_count += len(data.get('neg', []))\n",
    "        break\n",
    "\n",
    "neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'query': 119.96017897091723\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "total_length = 0\n",
    "count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        query = data.get('query', '')\n",
    "        total_length += len(query)\n",
    "        count += 1\n",
    "average_length = total_length / count if count > 0 else 0\n",
    "\n",
    "print(f\"Average length of 'query': {average_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'passage': 408.1756431767338\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "total_length = 0\n",
    "count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        for passage in data.get('pos', []):\n",
    "            total_length += len(passage)\n",
    "            count += 1\n",
    "\n",
    "        for passage in data.get('neg', []):\n",
    "            total_length += len(passage)\n",
    "            count += 1\n",
    "\n",
    "average_length = total_length / count if count > 0 else 0\n",
    "\n",
    "print(f\"Average length of 'passage': {average_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchrun --nproc_per_node 1 \\\n",
    "-m FlagEmbedding.baai_general_embedding.finetune.run \\\n",
    "--output_dir bge_large_fin \\\n",
    "--model_name_or_path BAAI/bge-large-en-v1.5 \\\n",
    "--train_data qa_finetune_data_minedHN.jsonl \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--dataloader_drop_last True \\\n",
    "--normlized True \\\n",
    "--temperature 0.02 \\\n",
    "--query_max_len 120 \\\n",
    "--passage_max_len 408 \\\n",
    "--train_group_size 5 \\\n",
    "--negatives_cross_device \\\n",
    "--logging_steps 100 \\\n",
    "--query_instruction_for_retrieval \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 285186/285186 [1:26:13<00:00, 55.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = FlagModel('bge_large_fin', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=False) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "index_name = 'abstracts_bge_fin1'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 1024\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    embedding = model.encode(chunk_text).tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from angle_emb import AnglE, AngleDataTokenizer, Prompts\n",
    "import json\n",
    "\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "angle.set_prompt(prompt=Prompts.C)\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        data = [{'text': json.loads(line)['query'], \n",
    "                        'positive': pos, \n",
    "                        'negative': neg} \n",
    "                for line in lines \n",
    "                for pos in json.loads(line)['pos'] \n",
    "                for neg in json.loads(line)['neg']]\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "train_ds = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "valid_ds = Dataset.from_pandas(pd.DataFrame(valid_data))\n",
    "test_ds = Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "\n",
    "train_ds = train_ds.shuffle().map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "valid_ds = valid_ds.map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "test_ds = test_ds.map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "\n",
    "# 4. fit\n",
    "angle.fit(\n",
    "    train_ds=train_ds,\n",
    "    valid_ds=valid_ds,\n",
    "    output_dir='ckpts/sts-b',\n",
    "    batch_size=8,\n",
    "    epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=100,\n",
    "    eval_steps=1000,\n",
    "    warmup_steps=0,\n",
    "    gradient_accumulation_steps=1,\n",
    "    loss_kwargs={\n",
    "        'w1': 1.0,\n",
    "        'w2': 1.0,\n",
    "        'w3': 1.0,\n",
    "        'cosine_tau': 20,\n",
    "        'ibn_tau': 20,\n",
    "        'angle_tau': 1.0\n",
    "    },\n",
    "    fp16=True,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# 5. evaluate\n",
    "corrcoef, accuracy = angle.evaluate(test_ds, device=angle.device)\n",
    "print('corrcoef:', corrcoef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
