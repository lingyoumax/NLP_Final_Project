{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = FlagModel('BAAI/bge-large-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=False) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "index_name = 'abstracts_bge'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 1024\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    embedding = model.encode(chunk_text).tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62372/62372 [14:44<00:00, 70.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import LLMEmbedder\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = LLMEmbedder('BAAI/llm-embedder', use_fp16=False)\n",
    "task = \"qa\"\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "index_name = 'abstracts_llm-embedder'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 768\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    embedding = model.encode_keys(chunk_text, task=task).tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "from angle_emb import AnglE, Prompts\n",
    "\n",
    "\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "angle.set_prompt(prompt=Prompts.C)\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data.csv')\n",
    "\n",
    "index_name = 'abstracts_uae'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 1024\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    vecs = angle.encode({'text': chunk_text}, to_numpy=True)\n",
    "    embedding = vecs[0].tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df_qa = pd.read_csv('qap.csv').drop('Answer', axis=1)\n",
    "data_list = df_qa.values.tolist()\n",
    "random.shuffle(data_list)\n",
    "data_list = data_list[:1000]\n",
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    vecs = angle.encode({'text': query}, to_numpy=True)\n",
    "    embedding = vecs[0].tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index=index_name, body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "neg_count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        neg_count += len(data.get('neg', []))\n",
    "        break\n",
    "\n",
    "neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'query': 119.96017897091723\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "total_length = 0\n",
    "count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        query = data.get('query', '')\n",
    "        total_length += len(query)\n",
    "        count += 1\n",
    "average_length = total_length / count if count > 0 else 0\n",
    "\n",
    "print(f\"Average length of 'query': {average_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'passage': 408.1756431767338\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "total_length = 0\n",
    "count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        for passage in data.get('pos', []):\n",
    "            total_length += len(passage)\n",
    "            count += 1\n",
    "\n",
    "        for passage in data.get('neg', []):\n",
    "            total_length += len(passage)\n",
    "            count += 1\n",
    "\n",
    "average_length = total_length / count if count > 0 else 0\n",
    "\n",
    "print(f\"Average length of 'passage': {average_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchrun --nproc_per_node 1 \\\n",
    "-m FlagEmbedding.baai_general_embedding.finetune.run \\\n",
    "--output_dir bge_large_fin \\\n",
    "--model_name_or_path BAAI/bge-large-en-v1.5 \\\n",
    "--train_data qa_finetune_data_minedHN.jsonl \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--dataloader_drop_last True \\\n",
    "--normlized True \\\n",
    "--temperature 0.02 \\\n",
    "--query_max_len 120 \\\n",
    "--passage_max_len 408 \\\n",
    "--train_group_size 5 \\\n",
    "--negatives_cross_device \\\n",
    "--logging_steps 100 \\\n",
    "--query_instruction_for_retrieval \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 285186/285186 [1:26:13<00:00, 55.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import pandas as pd\n",
    "from opensearchpy import OpenSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = FlagModel('bge_large_fin', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=False) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "index_name = 'abstracts_bge_fin1'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 1024\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    title = row['ArticleTitle']\n",
    "    authors_list = row[\"Authors\"].split(',')\n",
    "    authors = [author.strip().lower() for author in authors_list]\n",
    "    chunk_id = row['chunk_id']\n",
    "    chunk_text = row['chunk_text']\n",
    "    embedding = model.encode(chunk_text).tolist()\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": embedding,\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "        \"text_chunk_id\": chunk_id,\n",
    "        \"arxiv_text\": chunk_text,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from angle_emb import AnglE, AngleDataTokenizer, Prompts\n",
    "import json\n",
    "\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "angle.set_prompt(prompt=Prompts.C)\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        data = [{'text': json.loads(line)['query'], \n",
    "                        'positive': pos, \n",
    "                        'negative': neg} \n",
    "                for line in lines \n",
    "                for pos in json.loads(line)['pos'] \n",
    "                for neg in json.loads(line)['neg']]\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "train_ds = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "valid_ds = Dataset.from_pandas(pd.DataFrame(valid_data))\n",
    "test_ds = Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "\n",
    "train_ds = train_ds.shuffle().map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "valid_ds = valid_ds.map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "test_ds = test_ds.map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)\n",
    "\n",
    "# 4. fit\n",
    "angle.fit(\n",
    "    train_ds=train_ds,\n",
    "    valid_ds=valid_ds,\n",
    "    output_dir='ckpts/sts-b',\n",
    "    batch_size=8,\n",
    "    epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=100,\n",
    "    eval_steps=1000,\n",
    "    warmup_steps=0,\n",
    "    gradient_accumulation_steps=1,\n",
    "    loss_kwargs={\n",
    "        'w1': 1.0,\n",
    "        'w2': 1.0,\n",
    "        'w3': 1.0,\n",
    "        'cosine_tau': 20,\n",
    "        'ibn_tau': 20,\n",
    "        'angle_tau': 1.0\n",
    "    },\n",
    "    fp16=True,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# 5. evaluate\n",
    "corrcoef, accuracy = angle.evaluate(test_ds, device=angle.device)\n",
    "print('corrcoef:', corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4457/4457 [2:00:13<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "model = FlagModel('bge_large_fin', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=True)\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "pc = Pinecone(api_key=\"621f7574-8c97-4f46-8c5e-186dd099d33b\")\n",
    "\n",
    "pc.create_index(\n",
    "    name=\"bge-fin\",\n",
    "    dimension=1024, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-west-2\"\n",
    "    ) \n",
    ")\n",
    "\n",
    "index = pc.Index(\"bge-fin\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for start_idx in tqdm(range(0, df.shape[0], batch_size)):\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch = df.iloc[start_idx:end_idx]\n",
    "    vectors_to_upsert = []\n",
    "\n",
    "    for _, row in batch.iterrows():\n",
    "        pmid = str(row['PMID'])\n",
    "        chunk_id = str(row['chunk_id'])\n",
    "        unique_id = f\"{pmid}_{chunk_id}\"\n",
    "\n",
    "        chunk_text = row['chunk_text']\n",
    "        embedding = model.encode(chunk_text).tolist()\n",
    "\n",
    "        publishedDate = row['PubDate'] if pd.notna(row['PubDate']) and row['PubDate'].strip().lower() != 'unknown' else \"\"\n",
    "\n",
    "        vectors_to_upsert.append({\n",
    "            \"id\": unique_id,\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\n",
    "                \"pmid\": pmid,\n",
    "                \"title\": row['ArticleTitle'],\n",
    "                \"publishedDate\": publishedDate,  \n",
    "                \"authors\": [author.strip().lower() for author in row[\"Authors\"].split(',')],\n",
    "                \"text_chunk_id\": chunk_id,\n",
    "                \"arxiv_text\": chunk_text,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    if vectors_to_upsert:\n",
    "        index.upsert(vectors=vectors_to_upsert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In April 2019, the US Food and Drug Administration, in conjunction with 11 professional ophthalmic, vision science, and optometric societies, convened a forum on laser-based imaging.\\n\\nThe forum brought together the Food and Drug Administration, clinicians, researchers, industry members, and other stakeholders to stimulate innovation and ensure that patients in the US are the first in the world to have access to high-quality, safe, and effective medical devices.', '\"Actions\" are treated very generally, from mass protests to votes and other collective decisions, such as, e.g., acceptance (often unconscious) of some societal recommendations.\\n\\nIn this paper, we concentrate on the theory of laser resonators, physical vs. social.\\n\\nFor the latter, we analyze in detail the functioning of Internet-based echo chambers.\\n\\nTheir main purpose is increasing of the power of the quantum information field as well as its coherence.', 'Background & Aims: The United States Food and Drug Administration (FDA) regulates a broad range of consumer products, which account for about 25% of the United States market.\\n\\nThe FDA regulatory activities often involve producing and reading of a large number of documents, which is time consuming and labor intensive.']\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "model = FlagModel('bge_large_fin', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=True)\n",
    "\n",
    "pc = Pinecone(api_key=\"621f7574-8c97-4f46-8c5e-186dd099d33b\")\n",
    "index = pc.Index(\"bge-fin\")\n",
    "\n",
    "def search_arxiv_texts(query):\n",
    "    \n",
    "    query_vector = model.encode_queries([query])[0].tolist()\n",
    "    \n",
    "    response = index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=3,\n",
    "        include_metadata=True  \n",
    "    )\n",
    "\n",
    "    \n",
    "    arxiv_texts = [match['metadata']['arxiv_text'] for match in response['matches']]\n",
    "\n",
    "    return arxiv_texts\n",
    "\n",
    "\n",
    "query = \"What was the purpose of the US Food and Drug Administration-cosponsored forum on laser-based imaging?\"\n",
    "top_arxiv_texts = search_arxiv_texts(query)\n",
    "print(top_arxiv_texts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
