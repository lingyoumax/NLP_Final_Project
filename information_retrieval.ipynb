{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `docker compose up` to set up OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Search function in OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 58933/58933 [03:05<00:00, 318.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "index_name = 'abstracts'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "os_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 1,\n",
    "            \"knn\": True,\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"abstract\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "client.indices.create(index_name, body=os_mapping, ignore=400)\n",
    "df = pd.read_csv('data_cluster.csv').drop('Keywords', axis=1).drop('Cluster', axis=1)\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    abstract = row['Abstract'].replace(\"\\n\", \" \")\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"abstract\": abstract\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:07<00:00, 128.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PMID found in top 5 results for 74.6% of queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "df_qa = pd.read_csv('qap.csv').drop('Answer', axis=1)\n",
    "data_list = df_qa.values.tolist()\n",
    "data_list = data_list[:1000]\n",
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    res = client.search(index='abstracts', body={\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"abstract\": query\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    })\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly good result. However, we suspect that the questions we are currently generating might be too biased towards lexical search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1  \n",
    "According to[Semantic Search and Vector Databases (Part 2).ipynb](https://moodle.uni-heidelberg.de/pluginfile.php/1294993/mod_label/intro/Semantic%20Search%20and%20Vector%20Databases%20%28Part%202%29.ipynb?time=1701955836513), try to use BERT (bi-encoder) to compute the embeddings for the abstracts (No text splitting) and similarity scores are calculated by calculating the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 58933/58933 [17:03<00:00, 57.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from opensearchpy import OpenSearch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bert_version = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_version)\n",
    "model = BertModel.from_pretrained(bert_version)\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "index_name = 'abstracts_bert_nosplit'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 768\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "df = pd.read_csv('data_cluster.csv').drop('Keywords', axis=1).drop('Cluster', axis=1)\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    title = row['ArticleTitle']\n",
    "    abstract = row['Abstract'].replace(\"\\n\", \" \")\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    authors = list({term.lower() for term in row[\"Authors\"]})\n",
    "    \n",
    "    encoding = tokenizer(abstract, return_tensors='pt', truncation=True)\n",
    "    encoding = encoding.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "    output = output[0].mean(dim=1).squeeze().to('cpu')\n",
    "    client.index(index_name, {\n",
    "        \"pmid\": pmid,\n",
    "        \"title\": title,\n",
    "        \"vector\": output.numpy().tolist(),\n",
    "        \"publishedDate\": publishedDate,\n",
    "        \"authors\": authors,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 300/300 [00:32<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PMID found in top 10 results for 21.333333333333336% of queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    encoding = tokenizer(query, return_tensors='pt')\n",
    "    encoding = encoding.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "    output = output[0].mean(dim=1).squeeze().to('cpu')\n",
    "    body = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": output.numpy().tolist(),\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index='abstracts_bert_nosplit', body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 10 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this experiment are very poor. We overlooked the maximum input limitation of the BERT model. This might be the primary reason for such bad outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2  \n",
    "According to[Semantic Search and Vector Databases (Part 3).ipynb](https://moodle.uni-heidelberg.de/pluginfile.php/1299406/mod_label/intro/Semantic%20Search%20and%20Vector%20Databases%20%28Part%203%29.ipynb?time=1702551190164), try to use Sentence-BERT (SBERT) to compute the embeddings for the abstracts (text splitting) and similarity scores are calculated by calculating the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "embed_model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(embed_model_id)\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=embed_model_id,  # specify the model used for tokenization\n",
    "    chunk_overlap=10,  # set the overlap between consecutive text chunks\n",
    ")\n",
    "\n",
    "index_name = 'abstracts_sbert'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 768\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "df = pd.read_csv('data_cluster.csv').drop('Keywords', axis=1).drop('Cluster', axis=1)\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    title = row['ArticleTitle']\n",
    "    abstract = row['Abstract'].replace(\"\\n\", \" \")\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    authors = list({term.lower() for term in row[\"Authors\"]})\n",
    "    chunks = splitter.split_text(text=abstract)\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        embedding = model.encode(chunk).tolist()\n",
    "        client.index(index_name, {\n",
    "            \"pmid\": pmid,\n",
    "            \"title\": title,\n",
    "            \"vector\": embedding,\n",
    "            \"publishedDate\": publishedDate,\n",
    "            \"authors\": authors,\n",
    "            \"text_chunk_id\": j,\n",
    "            \"arxiv_text\": chunk,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:58<00:00,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PMID found in top 5 results for 54.900000000000006% of queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(embed_model_id)\n",
    "\n",
    "\n",
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    embedding = model.encode(query).tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index='abstracts_sbert', body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result this time is 55%. We believe the reason is due to the bias in the question search and the fact that it has not yet been fine-tuned according to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "fine-tuning Sentence-BERT (SBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 909/909 [5:17:08<00:00, 20.93s/it]\n",
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [5:17:08<00:00, 19028.90s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "training_df = pd.read_csv('qap.csv')\n",
    "abstracts_df = pd.read_csv('data_cluster.csv')\n",
    "merged_df = training_df.merge(abstracts_df, how='left', left_on='PMID', right_on='PMID')\n",
    "\n",
    "filtered_df = merged_df.dropna(subset=['Question', 'Abstract'])\n",
    "filtered_df = filtered_df[filtered_df['Question'].str.strip() != '']\n",
    "filtered_df = filtered_df[filtered_df['Abstract'].str.strip() != '']\n",
    "\n",
    "embed_model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(embed_model_id)\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=embed_model_id,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "training_examples = []\n",
    "for _, row in tqdm(filtered_df.iterrows(), total=filtered_df.shape[0]):\n",
    "    question = row['Question']\n",
    "    abstract = row['Abstract'].replace(\"\\n\", \" \")\n",
    "    abstract_chunks = splitter.split_text(text=abstract)\n",
    "    for chunk in abstract_chunks:\n",
    "        training_examples.append(InputExample(texts=[question, chunk], label=1.0))\n",
    "\n",
    "train_dataloader = DataLoader(training_examples, batch_size=16, shuffle=True)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n",
    "model.save('sbert_fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58933/58933 [21:47<00:00, 45.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from opensearchpy import OpenSearch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "embed_model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer('sbert_fin')\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name='sbert_fin',  # specify the model used for tokenization\n",
    "    chunk_overlap=10,  # set the overlap between consecutive text chunks\n",
    ")\n",
    "\n",
    "index_name = 'abstracts_sbert_fin_1'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 768\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "df = pd.read_csv('data_cluster.csv').drop('Keywords', axis=1).drop('Cluster', axis=1)\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    title = row['ArticleTitle']\n",
    "    abstract = row['Abstract'].replace(\"\\n\", \" \")\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    authors = list({term.lower() for term in row[\"Authors\"]})\n",
    "    chunks = splitter.split_text(text=abstract)\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        embedding = model.encode(chunk).tolist()\n",
    "        client.index(index_name, {\n",
    "            \"pmid\": pmid,\n",
    "            \"title\": title,\n",
    "            \"vector\": embedding,\n",
    "            \"publishedDate\": publishedDate,\n",
    "            \"authors\": authors,\n",
    "            \"text_chunk_id\": j,\n",
    "            \"arxiv_text\": chunk,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:37<00:00,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PMID found in top 5 results for 15.666666666666668% of queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_5_hits = 0\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    embedding = model.encode(query).tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index='abstracts_sbert_fin_1', body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment failed because only positive samples were generated and no negative samples when preparing the dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4\n",
    "fine-tuning Sentence-BERT (SBERT) with negative sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12254/12254 [02:18<00:00, 88.40it/s]\n",
      "Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2440/2440 [13:01<00:00,  3.12it/s]\n",
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [13:01<00:00, 781.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "import random\n",
    "\n",
    "training_df = pd.read_csv('qap.csv')\n",
    "abstracts_df = pd.read_csv('data_cluster.csv')\n",
    "merged_df = training_df.merge(abstracts_df, how='left', left_on='PMID', right_on='PMID')\n",
    "\n",
    "filtered_df = merged_df.dropna(subset=['Question', 'Abstract'])\n",
    "filtered_df = filtered_df[filtered_df['Question'].str.strip() != '']\n",
    "filtered_df = filtered_df[filtered_df['Abstract'].str.strip() != '']\n",
    "\n",
    "embed_model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(embed_model_id)\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=embed_model_id,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "training_examples = []\n",
    "for _, row in tqdm(filtered_df.iterrows(), total=filtered_df.shape[0]):\n",
    "    question = row['Question']\n",
    "    correct_abstract = row['Abstract'].replace(\"\\n\", \" \")\n",
    "    correct_chunks = splitter.split_text(text=correct_abstract)\n",
    "\n",
    "    for chunk in correct_chunks:\n",
    "        training_examples.append(InputExample(texts=[question, chunk], label=1.0))\n",
    "\n",
    "    for _ in range(2):\n",
    "        incorrect_abstract = filtered_df[filtered_df['Abstract'] != correct_abstract].sample(1)['Abstract'].iloc[0].replace(\"\\n\", \" \")\n",
    "        incorrect_chunks = splitter.split_text(text=incorrect_abstract)\n",
    "\n",
    "        if incorrect_chunks:\n",
    "            incorrect_chunk = random.choice(incorrect_chunks)\n",
    "            training_examples.append(InputExample(texts=[question, incorrect_chunk], label=0.0))\n",
    "\n",
    "train_dataloader = DataLoader(training_examples, batch_size=16, shuffle=True)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n",
    "model.save('sbert_fin_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58933/58933 [21:58<00:00, 44.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from opensearchpy import OpenSearch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SentenceTransformer('sbert_fin_2')\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name='sbert_fin_2',  # specify the model used for tokenization\n",
    "    chunk_overlap=10,  # set the overlap between consecutive text chunks\n",
    ")\n",
    "\n",
    "index_name = 'abstracts_sbert_fin_2'\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": 'opensearch', \"port\": 9200}],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"pmid\": {\n",
    "                \"type\": \"keyword\",  \n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",  \n",
    "            },\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",  \n",
    "                \"dimension\": 768\n",
    "            },\n",
    "            \"publishedDate\": {\n",
    "                \"type\": \"date\",  # date type for publication date\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"text\",  # text field for author names\n",
    "            },\n",
    "            \"text_chunk_id\": {\n",
    "                \"type\": \"integer\",\n",
    "            },\n",
    "            \"arxiv_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.indices.create(index_name, body=mapping, ignore=400)\n",
    "\n",
    "df = pd.read_csv('data_cluster.csv').drop('Keywords', axis=1).drop('Cluster', axis=1)\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    pmid = row['PMID']\n",
    "    title = row['ArticleTitle']\n",
    "    abstract = row['Abstract'].replace(\"\\n\", \" \")\n",
    "    publishedDate = row['PubDate']\n",
    "    if pd.isna(publishedDate) or publishedDate.strip().lower() == 'unknown':\n",
    "        publishedDate = None\n",
    "    authors = list({term.lower() for term in row[\"Authors\"]})\n",
    "    chunks = splitter.split_text(text=abstract)\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        embedding = model.encode(chunk).tolist()\n",
    "        client.index(index_name, {\n",
    "            \"pmid\": pmid,\n",
    "            \"title\": title,\n",
    "            \"vector\": embedding,\n",
    "            \"publishedDate\": publishedDate,\n",
    "            \"authors\": authors,\n",
    "            \"text_chunk_id\": j,\n",
    "            \"arxiv_text\": chunk,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:59<00:00,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PMID found in top 5 results for 62.4% of queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_5_hits = 0\n",
    "\n",
    "model = SentenceTransformer('sbert_fin_2')\n",
    "\n",
    "for query, correct_pmid in tqdm(data_list):\n",
    "    embedding = model.encode(query).tolist()\n",
    "    body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"vector\",\n",
    "                        \"query_value\": embedding,\n",
    "                        \"space_type\": \"cosinesimil\"\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = client.search(index='abstracts_sbert_fin_2', body=body)\n",
    "    search_results_pmids = [hit['_source']['pmid'] for hit in res['hits']['hits']]\n",
    "    if correct_pmid in search_results_pmids:\n",
    "        top_5_hits += 1\n",
    "percentage_top_5 = (top_5_hits / len(data_list)) * 100\n",
    "print(f\"Correct PMID found in top 5 results for {percentage_top_5}% of queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "62% of the questions were matched with abstracts. However, this is still lower than the results from OpenSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps. A few ideas for improving results:\n",
    "- Fine-tuning for a few more rounds, but being cautious of overfitting\n",
    "- trying other models\n",
    "- performing named entity recognition on the questions to increase the weight of important entities\n",
    "- using LLM to rewrite the questions before inputting them into the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
