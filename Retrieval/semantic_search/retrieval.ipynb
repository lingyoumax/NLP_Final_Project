{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "neg_count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        neg_count += len(data.get('neg', []))\n",
    "        break\n",
    "\n",
    "neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'query': 119.96017897091723\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "total_length = 0\n",
    "count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        query = data.get('query', '')\n",
    "        total_length += len(query)\n",
    "        count += 1\n",
    "average_length = total_length / count if count > 0 else 0\n",
    "\n",
    "print(f\"Average length of 'query': {average_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'passage': 408.1756431767338\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "total_length = 0\n",
    "count = 0\n",
    "\n",
    "with open('qa_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        for passage in data.get('pos', []):\n",
    "            total_length += len(passage)\n",
    "            count += 1\n",
    "\n",
    "        for passage in data.get('neg', []):\n",
    "            total_length += len(passage)\n",
    "            count += 1\n",
    "\n",
    "average_length = total_length / count if count > 0 else 0\n",
    "\n",
    "print(f\"Average length of 'passage': {average_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchrun --nproc_per_node 1 \\\n",
    "-m FlagEmbedding.baai_general_embedding.finetune.run \\\n",
    "--output_dir bge_large_fin \\\n",
    "--model_name_or_path BAAI/bge-large-en-v1.5 \\\n",
    "--train_data qa_finetune_data_minedHN.jsonl \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--dataloader_drop_last True \\\n",
    "--normlized True \\\n",
    "--temperature 0.02 \\\n",
    "--query_max_len 120 \\\n",
    "--passage_max_len 408 \\\n",
    "--train_group_size 5 \\\n",
    "--negatives_cross_device \\\n",
    "--logging_steps 100 \\\n",
    "--query_instruction_for_retrieval \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4457/4457 [2:00:13<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "model = FlagModel('bge_large_fin', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=True)\n",
    "\n",
    "df = pd.read_csv('splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "pc = Pinecone(api_key=\"621f7574-8c97-4f46-8c5e-186dd099d33b\")\n",
    "\n",
    "pc.create_index(\n",
    "    name=\"bge-fin\",\n",
    "    dimension=1024, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-west-2\"\n",
    "    ) \n",
    ")\n",
    "\n",
    "index = pc.Index(\"bge-fin\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for start_idx in tqdm(range(0, df.shape[0], batch_size)):\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch = df.iloc[start_idx:end_idx]\n",
    "    vectors_to_upsert = []\n",
    "\n",
    "    for _, row in batch.iterrows():\n",
    "        pmid = str(row['PMID'])\n",
    "        chunk_id = str(row['chunk_id'])\n",
    "        unique_id = f\"{pmid}_{chunk_id}\"\n",
    "\n",
    "        chunk_text = row['chunk_text']\n",
    "        embedding = model.encode(chunk_text).tolist()\n",
    "\n",
    "        publishedDate = row['PubDate'] if pd.notna(row['PubDate']) and row['PubDate'].strip().lower() != 'unknown' else \"\"\n",
    "\n",
    "        vectors_to_upsert.append({\n",
    "            \"id\": unique_id,\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\n",
    "                \"pmid\": pmid,\n",
    "                \"title\": row['ArticleTitle'],\n",
    "                \"publishedDate\": publishedDate,  \n",
    "                \"authors\": [author.strip().lower() for author in row[\"Authors\"].split(',')],\n",
    "                \"text_chunk_id\": chunk_id,\n",
    "                \"arxiv_text\": chunk_text,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    if vectors_to_upsert:\n",
    "        index.upsert(vectors=vectors_to_upsert)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
