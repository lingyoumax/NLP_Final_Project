{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f3346fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "model = FlagModel('bge_large_fin',\n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=True)\n",
    "\n",
    "pc = Pinecone(api_key=\"621f7574-8c97-4f46-8c5e-186dd099d33b\")\n",
    "Index = pc.Index(\"bge-fin\")\n",
    "\n",
    "\n",
    "def search_arxiv_texts(query):\n",
    "    query_vector = model.encode_queries([query])[0].tolist()\n",
    "\n",
    "    response = Index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=5,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    arxiv_texts = [str(match['metadata']['arxiv_text']) for match in response['matches']]\n",
    "    pmid = [int(match['metadata']['pmid']) for match in response['matches']]\n",
    "    ans = {}\n",
    "    for i in range(5):\n",
    "        ans[pmid[i]] = arxiv_texts[i]\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "# query = \"What was the purpose of the US Food and Drug Administration-cosponsored forum on laser-based imaging?\"\n",
    "# top_arxiv_texts = search_arxiv_texts(query)\n",
    "# print(top_arxiv_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6b95c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from typing import List\n",
    "import yake\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "kw_extractor = yake.KeywordExtractor(n=1,\n",
    "                                     dedupLim=0.9,\n",
    "                                     top=10,\n",
    "                                     features=None)\n",
    "\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, corpus: List[List[str]], k1=1.5, b=0.95):\n",
    "        self.corpus = corpus\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.documents_number = len(corpus)\n",
    "        self.avgdl = sum(len(document) for document in corpus) / self.documents_number\n",
    "        self.df = self._calculate_df()\n",
    "        self.idf = self._calculate_idf()\n",
    "\n",
    "    def _calculate_df(self):\n",
    "        df = {}\n",
    "        for document in self.corpus:\n",
    "            for word in set(document):\n",
    "                df[word] = df.get(word, 0) + 1\n",
    "        return df\n",
    "\n",
    "    def _calculate_idf(self):\n",
    "        idf = {}\n",
    "        for word, freq in self.df.items():\n",
    "            idf[word] = math.log((self.documents_number - freq + 0.5) / (freq + 0.5) + 1)\n",
    "        return idf\n",
    "\n",
    "    def _score(self, document, query):\n",
    "        score = 0.0\n",
    "        for word in query:\n",
    "            if word in self.df:\n",
    "                idf = self.idf[word]\n",
    "                term_freq = document.count(word)\n",
    "                score += (idf * term_freq * (self.k1 + 1)) / (\n",
    "                        term_freq + self.k1 * (1 - self.b + self.b * len(document) / self.avgdl))\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        scores = []\n",
    "        for index, document in enumerate(self.corpus):\n",
    "            score = self._score(document, query)\n",
    "            scores.append((index, score))\n",
    "        return scores\n",
    "\n",
    "\n",
    "def search(query: str, df, keywords, bm25, top_k):\n",
    "    tokenized_query = query.split()\n",
    "    tokenized_query.extend(keywords)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    result = {}\n",
    "    for doc_index, score in sorted_scores:\n",
    "        pmid = df.iloc[doc_index]['PMID']\n",
    "        result[pmid] = score\n",
    "    return list(result.keys())\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    keywords_set = {word for word, _ in keywords}\n",
    "    return list(set(keywords_set))\n",
    "\n",
    "\n",
    "def query_pre_process(query):\n",
    "    proper_nouns = extract_keywords(query)\n",
    "    proper_nouns = [word for word in proper_nouns if word.lower() not in ENGLISH_STOP_WORDS] * 1\n",
    "    return proper_nouns\n",
    "\n",
    "df = pd.read_csv('./PubmedDataSet.csv')\n",
    "texts = df['Abstract'].tolist()\n",
    "tokenized_texts = [doc.split() for doc in texts]\n",
    "bm25_abstract = BM25(tokenized_texts)\n",
    "df2 = pd.read_csv('./splitted_pubmed_data_NLTK.csv')\n",
    "\n",
    "def weightBM25(query):\n",
    "    keywords = query_pre_process(query)\n",
    "    result_pmids_scores = search(query, df, keywords, bm25_abstract, top_k=30)\n",
    "    mask = df2['PMID'].isin(result_pmids_scores)\n",
    "    df_t = df2[mask]\n",
    "    texts = df_t['chunk_text'].tolist()\n",
    "    tokenized_texts = [doc.split() for doc in texts]\n",
    "    \n",
    "    bm25_chunk = BM25(tokenized_texts)\n",
    "    result_pmids_scores = search(query, df_t, keywords, bm25_chunk, top_k=5)\n",
    "    \n",
    "    res = {}\n",
    "    for id in result_pmids_scores:\n",
    "        res[id] = df_t[df_t[\"PMID\"] == id]['chunk_text'].iloc[0]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48dceb56cbd5e1f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def QWEN_score(sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    cos similarity of two sentences\n",
    "    :param sentence1: string\n",
    "    :param sentence2: string\n",
    "    :return: number\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    inputs1 = tokenizer(sentence1, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    inputs2 = tokenizer(sentence2, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1).last_hidden_state\n",
    "        outputs2 = model(**inputs2).last_hidden_state\n",
    "\n",
    "    features1 = outputs1[:, 0, :]\n",
    "    features2 = outputs2[:, 0, :]\n",
    "\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(features1, features2)\n",
    "    print(cosine_similarity.item())\n",
    "    return cosine_similarity.item()\n",
    "    \n",
    "def filter(query, res):\n",
    "    ans_pmid = 0\n",
    "    max_cos = -1.0\n",
    "    for pmid, chunk in res.items():\n",
    "        t = QWEN_score(query, chunk)\n",
    "        if t > max_cos:\n",
    "            max_cos = t\n",
    "            ans_pmid = pmid\n",
    "    return max_cos,ans_pmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7211cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(question):    \n",
    "    res_semantic = search_arxiv_texts(question)\n",
    "    cos1,res_semantic_pmid = filter(question, res_semantic)\n",
    "    \n",
    "    \n",
    "    res_lex = weightBM25(question)\n",
    "    res_lex_pmid = filter(question, res_lex)\n",
    "\n",
    "    # if res_lex_pmid == []:\n",
    "    #     return list(res_lex.keys())[0]\n",
    "    # return res_lex_pmid[0]\n",
    "    \n",
    "    # intersectChunk = list(set(res_semantic).intersection(set(res_lex)))\n",
    "    # if len(intersectChunk) != 0:\n",
    "    #     for id in res_semantic:\n",
    "    #         if id in intersectChunk:\n",
    "    #             return id\n",
    "    #             break\n",
    "    # else:\n",
    "    #     return [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "233e3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df_test = pd.read_csv('./evaluation.csv')\n",
    "test = []\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    small_list = [row[\"Question\"], row[\"PMID\"]]\n",
    "    test.append(small_list)\n",
    "random.seed(42)\n",
    "random.shuffle(test)\n",
    "test = test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe2f2de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                              | 0/10 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 10%|███████████████                                                                                                                                       | 1/10 [00:03<00:31,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314301371574402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9903689622879028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 20%|██████████████████████████████                                                                                                                        | 2/10 [00:09<00:37,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824298024177551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937097430229187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9541096687316895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 30%|█████████████████████████████████████████████                                                                                                         | 3/10 [00:15<00:38,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9541096687316895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8655787706375122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9072640538215637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9520996809005737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 40%|████████████████████████████████████████████████████████████                                                                                          | 4/10 [00:24<00:42,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852910041809082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 50%|███████████████████████████████████████████████████████████████████████████                                                                           | 5/10 [00:28<00:28,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7115401029586792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314301371574402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314301371574402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9072640538215637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8617032766342163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████                                                            | 6/10 [00:40<00:31,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8617032766342163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8717578649520874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8717578649520874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9322625994682312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                             | 7/10 [00:50<00:25,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8686384558677673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966816782951355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8163858652114868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9857695698738098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9226570129394531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                              | 8/10 [01:01<00:18,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8275462985038757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314301371574402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314301371574402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926376461982727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9614694714546204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████               | 9/10 [01:11<00:09,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8218483328819275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9350156188011169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:16<00:00,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9620165824890137\n",
      "recall of mixed search algorithm 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for question, pmid in tqdm(test):\n",
    "    top_pmids = retrieval(question)\n",
    "    if pmid == top_pmids:\n",
    "        t += 1\n",
    "print(f'recall of mixed search algorithm {t / len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9dbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5aac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
